# use for any midi drum note (hi hats, toms, etc.) 

# training data revision
- use loudnorm filter in ffmpeg
- save velocities
  - velocity 
  - channel volume controls
- shorter chunks (8 seconds)
- teacher forcing (linear ramp up)
- mono direkt bei ffmpeg

// 

- D: Try v2Funnel nochmal.

- D(nicht so gut): make funnels for audio events 
- D(niedrige learn rate war wichtig): schedule learn rate 0.001 => 0.00001 (over 50 epochs) 

//

- D: when training save model every x epochs
- D: weight buffers with events higher than others
- D: observe how it decreases to 0

Experimente
- D: Mono (Performance vorteil?) 

3) Application in the Browser

2) Training

- D: Exit when change in loss < threshold

- D: Use Pytorch for defining the model and training
- Try different architectures programmatically
  - cnn layers, features
  - rnn layers and size
  - training parameters
  - each to 20 epochs
  - then compare the 5 best in depth 
- Convert to ONNX 
- Use it in the browser with onnxruntime-web

1) Ground Truth

- Use data from Nighthawks annotated manually for fine tuning

- D: Damn! the ffmpeg speed filter I am using does not pitch down at the moment
  - D: Need to use samplerate instead

- D: Use data created from MIDI files for training
  - Keep the specific notes and programs (general MIDI sound set) variable
    - For a start
      - kick and 
      - snare
      - (flash can be overall FFT engergy or so) 
  - x2 by using different sound fonts

- D(more or less): In both cases: 
  - x5 by pitching up and down 2x
  - x3 by adding reverb
  - x3 by adding noise
    - white noise
    - some noise

- D: Cut out 10s pieces
  - sometimes combine one song with another
  - audio and kicks.txt and snare.txt

